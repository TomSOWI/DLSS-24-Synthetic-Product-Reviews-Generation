# **Replication material and Appendix for the paper "Synthetic Product Review Generation for Market Analysis: A GPT-2 and LLaMA 3 Approach"**
**Description:** This repository contains the replication materials and the Appendix for our final project for the course "Deep Learning for the Social Sciences" (SuSe 2024) at the University of Konstanz.

**Authors:** Carl George-Lembach, Tom Klotz, Julia Schlei√üheimer, and Valentin Velev

## Appendix
The Appendix for our paper is available in the file [appendix.pdf]() :page_facing_up:.

## Fine-Tuned GPT-2 Medium
Our fine-tuned GPT-2 Medium model is available on [Hugging Face](https://huggingface.co/TomData/GPT2-review) :hugs:.

Nore that since we used PyTorch to fine-tune GPT-2 Medium, the Inference API on Hugging Face does not work.

## Data
In the folder [Data](https://github.com/TomSOWI/DLSS-24-Synthetic-Product-Reviews-Generation/tree/main/Data), we have made available our main datasets. The remaining datasets (from the ETL Notebook) are available on [Synology Cloud](https://T34278926.quickconnect.to/d/s/zpVAefWwFEYfIhTRTc0RfJ1h4rXzh6kJ/7VRz2eFaGxxjR11Xtygq65lAszhLPaIi-7LuAL9qlnQs) :cloud:. Below you will find a brief overview of all the data files:

**Main Datasets:**

* [preprocessed_reviews.csv](): Contains our preprocessed data with 100,000 Amazon fashion item reviews
* [preprocessed_reviews_topics.parquet](): Contains ...
* [topic_modeling_results.csv](): Contains the results of BERTopic on the 100,000 Amazon product reviews

**Generated Reviews:**
* [gpt-2_synthetic_reviews.csv](): Contains the sample of 10,000 synthetic product reviews generated by our fine-tuned GPT-2 Medium and the perplexity scores for each synthetic product review
* [gpt-2_synthetic_reviews_human_eval_sample.csv](): Contains a random sample of 100 synthetic product reviews generated by our fine-tuned GPT-2 Medium for human evaluation and the perplexity scores for each synthetic product review
* [gpt-2_tested_prompts.csv](): Contains ...
* [llama-3_synthetic_reviews.csv](): Contains the sample of 10,000 synthetic product reviews generated by Llama 3 8B and the perplexity scores for each synthetic product review
* [llama-3_synthetic_reviews_human_eval_sample.csv](): Contains a random sample of 100 synthetic product reviews generated by our fine-tuned GPT-2 Medium for human evaluation and the perplexity scores for each synthetic product review
* [llama-3_tested_prompts.csv](): Contains ...

**Manual Evaluation:**
* [human_evaluation_rater1.xlsx](): Contains the results from rater 1 for the human evaluation
* [human_evaluation_rater2.xlsx](): Contains the results from rater 2 for the human evaluation
* [human_evaluation_rater3.xlsx](): Contains the results from rater 3 for the human evaluation
* [human_evaluation_rater4.xlsx](): Contains the results from rater 4 for the human evaluation
* [market_analysis_rater1.xlsx](): Contains the results from rater 1 for our market analysis
* [market_analysis_rater2.xlsx](): Contains the results from rater 2 for our market analysis

## Notebooks
The Jupyter Notebooks we used for all our computation, analysis, and visualization are available in mutliple (sub)directories. The main Notebooks can be found in the [Main Branch](). The Notebooks used for the intial classification (sentiment, topics) and in the folder [Classification](). The Notebooks used for generating the synthetic product review are available in [Review Generation](). Finally, the Notebooks for the analysis of the synthetic product reviews are in [Review Analysis](). Below you will find a brief overview of all the Jupyter Notebooks:

**Main Notebooks:**
* [ETL.ipynb](): Notebook for data preprocessing / extract-transform-load (ETL) pipeline including:
  * Data acquisition
  * Data cleaning
  * Preparation of final dataset
* [EDA.ipynb](): Notbook for exploratory data analysis (EDA) including:
  * Average length of reviews and standard deviation of review length
  * Histogram of review length distribution
  * Rating distribution (overall and by sentiment)
  * Word clouds (overall, by sentiment, and by rating)
  * Topic modeling using BERTopic
* [gpt-2_fine-tune.ipynb](): Notebook for fine-tuning GPT-2. Note that the fine-tuning with 100,000 reviews takes roughly 2-3 days using two NVIDIA A40 GPUs (48 GB).

**Notebooks for Classification:**
* [sentiment.ipynb](): Notebook for classifying the Amazon fashion item reviews to randomly sample by sentiment using SiEBERT (see paper for more information)
* [topic_modeling.ipynb](): Notebook for topic modelling using BERTopic

**Notebooks for Review Generation:**
* [gpt-2_review_generation.ipynb](): Notebook for generating synthetic product reviews using GPT-2 Medium and calculating perplexity.
* [llama-3_review_generation.ipynb](): Notebook for generating synthetic product reviews using LLaMA 3 8B and calculating perplexity. Note that the initialization of LLaMA 3 8B in Kaggle with an NVIDIA P100 GPU (16GB) takes roughly 50 minutes and the generation of 10,000 synthetic reviews takes roughly 20 hours.

**Notebooks for Review Analysis:**
* [stat_evaluation.ipynb](): Notebook for the statistical evaluation of the synthetic product reviews, including:
  * Statistical tendencies (Zipf's law)
  * Linguistic features (average length, type-token ratio, verb and noun usage, and readability)
  * MAUVE
  * Inter-rater reliability (for human evaluation)
* [market_analysis_part1.ipynb](): First Notebook for the market analysis, including:
  * ...
  * ...
* [market_analysis_part2.ipynb](): Second Notebook for the market analysis, including:
  * Sentiment analysis on 10,000 sample of real reviews and both samples of synthetic reviews 
  * Bar chart of the sentiments by review type (Real, GPT-2, LLaMA 3)

## Plots
In the folder [Plots](https://github.com/TomSOWI/DLSS-24-Synthetic-Product-Reviews-Generation/tree/main/Plots), we have uploaded all our plots. Below you will find a brief overview of all plots:

**Plots from the Paper:**
* [rating_distribution.pdf](): Figure 1 (left side) &ndash; Rating distribution
* [rating_distribution_sentiment.pdf](): Figure 1 (right side) &ndash; Rating distribution by sentiment
* [wordcloud.pdf](): Figure 2 &ndash; Word cloud of all real reviews
* [sentiment_distribution_review_types.pdf](): Figure 3 &ndash; Distribution of sentiment by review type (Real, GPT-2, LLaMA 3)

**Plots from the Appendix:**
* [hist_rev_len.pdf](): Figure A1 &ndash; Review length distribution
* [wordcloud_pos.pdf](): Figure A2 (top) &ndash; Word cloud of reviews classified as positive 
* [worcloud_neg.pdf](): Figure A2 (bottom) &ndash; Word cloud of reviews classified as negative
* [wordcloud_rating1.pdf](): Figure A3 (top left) &ndash; Word cloud of reviews with a rating 1/5
* [wordcloud_rating2.pdf](): Figure A3 (top right) &ndash; Word cloud of reviews with a rating 2/5
* [wordcloud_rating3.pdf](): Figure A3 (middle left) &ndash; Word cloud of reviews with a rating 3/5
* [wordcloud_rating4.pdf](): Figure A3 (middle right) &ndash; Word cloud of reviews with a rating 4/5
* [wordcloud_rating5.pdf](): Figure A3 (bottom) &ndash; Word cloud of reviews with a rating 5/5
