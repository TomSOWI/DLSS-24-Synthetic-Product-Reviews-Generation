{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:13.515846Z","iopub.status.busy":"2024-06-25T18:31:13.515467Z","iopub.status.idle":"2024-06-25T18:31:25.516096Z","shell.execute_reply":"2024-06-25T18:31:25.514986Z","shell.execute_reply.started":"2024-06-25T18:31:13.515800Z"},"trusted":true},"source":["# **GPT-2 Fine-Tuning**\n","\n","This Notebook performs the fine-tuning of GPT2 Medium on our preprocessed dataset (see [here](https://github.com/TomSOWI/DLSS-24-Synthetic-Product-Reviews-Generation/blob/main/Data/preprocessed_reviews.parquet)):\n","\n","* Check if chunking/truncation of reviews is required\n","* Assess tokenizer parameters\n","* Define PyTorch Dataset\n","* Fine-tuning\n","* Display model info\n","* Save model\n","* Show training loss\n","\n","## Preparations"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Import necessary packages\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2LMHeadModel, set_seed\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Golbal variables\n","SEED_VAL = 42\n","set_seed(SEED_VAL)\n","\n","# Define model related parameters\n","MODEL_NAME = \"gpt2-medium\"\n","BOS_TOKEN = '<|startoftext|>'\n","EOS_TOKEN = '<|endoftext|>'\n","PAD_TOKEN = '<|pad|>'\n","MAX_LENGTH = 1024"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Load data for training\n","trainset = pd.read_parquet(\"Data/preprocessed_reviews.parquet\")\n","\n","# Move reviews to a list for analysis and training\n","texts=trainset['text'].to_list()"]},{"cell_type":"markdown","metadata":{},"source":["## Analyze Review Length"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:32.033855Z","iopub.status.busy":"2024-06-25T18:31:32.033591Z","iopub.status.idle":"2024-06-25T18:31:32.609476Z","shell.execute_reply":"2024-06-25T18:31:32.608355Z","shell.execute_reply.started":"2024-06-25T18:31:32.033833Z"},"trusted":true},"outputs":[],"source":["tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n","# Store document length based on GPT2Tokenizer tokenization\n","doc_lengths = []\n","\n","for text in texts:\n","    # get list of token ids for one document\n","    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=False, padding=False)\n","    # append len of list to doc lengths\n","    doc_lengths.append(len(tokens))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Max documen length:  982\n","Average document length:  35.68134\n","Number of ducuments outside of gpt-mediums context size:  0\n"]}],"source":["print(\"Max documen length: \", np.max(doc_lengths))\n","print(\"Average document length: \", np.average(doc_lengths))\n","# No document outside the context size\n","print(\"Number of ducuments outside of gpt-mediums context size: \", len(doc_lengths[doc_lengths > MAX_LENGTH]))"]},{"cell_type":"markdown","metadata":{},"source":["Since no documents are outside of the optimal context size of 1024. Therefore we do not have to use any chunking technique"]},{"cell_type":"markdown","metadata":{},"source":["## Assess GPT2-Tokenizer Parameters"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Base BOS: <|endoftext|>\n","Base EOS: <|endoftext|>\n","Base PAD: None\n"]}],"source":["tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","print(f'Base BOS: {tokenizer.bos_token}')\n","print(f'Base EOS: {tokenizer.eos_token}')\n","print(f'Base PAD: {tokenizer.pad_token}')\n","\n","# adjust global Variables according to models naming of special tokens"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:32.632490Z","iopub.status.busy":"2024-06-25T18:31:32.631822Z","iopub.status.idle":"2024-06-25T18:31:33.814137Z","shell.execute_reply":"2024-06-25T18:31:33.813129Z","shell.execute_reply.started":"2024-06-25T18:31:32.632462Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The max model length is 1024 for this model\n","The beginning of sequence token <|startoftext|> token has the id 50257\n","The end of sequence token <|endoftext|> has the id 50256\n","The padding token <|pad|> has the id 50258\n"]}],"source":["# Load the GPT tokenizer.\n","tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN) \n","\n","print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n","print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n","print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n","print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"]},{"cell_type":"markdown","metadata":{},"source":["## Define PyTorch Dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:33.818164Z","iopub.status.busy":"2024-06-25T18:31:33.817832Z","iopub.status.idle":"2024-06-25T18:31:33.826954Z","shell.execute_reply":"2024-06-25T18:31:33.826021Z","shell.execute_reply.started":"2024-06-25T18:31:33.818136Z"},"trusted":true},"outputs":[],"source":["class TextDataset(Dataset):\n","\n","  def __init__(self, txt_list, tokenizer, max_length):\n","    # Store features\n","    self.tokenizer = tokenizer\n","    self.input_ids = []\n","    self.attn_masks = []\n","\n","    for txt in txt_list:\n","      # Encode text in between BOS and EOS\n","      encodings_dict = tokenizer(BOS_TOKEN + txt + EOS_TOKEN, truncation=True, max_length=max_length, padding=\"max_length\") # Set truncation just in case\n","\n","      # Make features tensors and store\n","      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","    \n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.attn_masks[idx]"]},{"cell_type":"markdown","metadata":{},"source":["## Fine-Tuning"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Set environment variables depending on the available GPU\n","# Use GPU 0\n","# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","#torch.cuda.set_device(0)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pop544167/.pyenv/versions/3.11.4/envs/project2/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1: 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]\n","Epoch 2: 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n","Epoch 3: 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n","Epoch 4: 100%|██████████| 50/50 [00:30<00:00,  1.65it/s]\n","Epoch 5: 100%|██████████| 50/50 [00:30<00:00,  1.67it/s]\n"]}],"source":["def fine_tune_gpt2(train_data):\n","    \"\"\"\n","    Fine-tunes a pre-trained GPT-2 model on a given dataset.\n","\n","    Parameters:\n","    -----------\n","    train_data : list of str\n","        A list of text sequences to be used for training the model.\n","\n","    Returns:\n","    --------\n","    args : dict\n","        A dictionary containing the hyperparameters and settings used during training, \n","        including model name, device information, batch size, learning rate, etc.\n","\n","    training_stats : list of dict\n","        A list of dictionaries, each containing the training loss for an epoch. \n","\n","    model : GPT2LMHeadModel\n","        The fine-tuned GPT-2 model\n","    \"\"\"\n","\n","    # Initialize list to store stats for each epoch\n","    training_stats = [] \n","    # Initialize the args dictionary to hold hyperparameters\n","    args = {}  \n","    \n","    # Set the seed value all over the place to make this reproducible.\n","    set_seed(SEED_VAL)\n","    \n","    # Update args with seed and model name\n","    args['seed_val'] = SEED_VAL\n","    args['model_name'] = MODEL_NAME\n","    \n","    # Load GPT-2 model and tokenizer\n","    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN) # add special tokens \n","    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n","    model.resize_token_embeddings(len(tokenizer)) # resize embeddings according to the updated tokenizer\n","\n","    # Update args with tokenizer and model hyperparameters\n","    args['bos_token'] = '50257'  # Beginning of sentence token ID\n","    args['eos_token'] = '50256'  # End of sentence token ID\n","    args['pad_token'] = '50258'  # Padding token ID\n","    args['max_length'] = MAX_LENGTH  # Maximum length of input sequences\n","    args['batch_size'] = 2  # Batch size for training\n","    args['learning_rate'] = 5e-4  # Learning rate for the optimizer\n","    args['eps'] = 1e-8  # Epsilon value to prevent division by zero in the optimizer\n","    args['num_train_epochs'] = 5  # Number of epochs for training\n","    args['num_warmup_steps'] = 1e2  # Number of warmup steps for the learning rate scheduler\n","    \n","    # Load training dataset\n","    train_dataset = TextDataset(\n","        tokenizer=tokenizer,\n","        txt_list=train_data,\n","        max_length=args['max_length']\n","    )\n","\n","    # Create data loader\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=RandomSampler(train_dataset),\n","        batch_size=args['batch_size']\n","    )\n","\n","    # Set up optimizer and learning rate scheduler\n","    optimizer = AdamW(model.parameters(), lr=args['learning_rate'], eps=args['eps'])\n","\n","     # Calculate number of total steps\n","    total_steps = len(train_dataloader) * args['num_train_epochs']\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=args['num_warmup_steps'],\n","        num_training_steps=total_steps\n","    )\n","\n","    # Move model to GPU if available\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    # Save used device to args\n","    args['device'] = device\n","    \n","    # Training loop\n","    # Set model to training mode\n","    model.train()\n","    # Iterate over each epoch\n","    for epoch in range(args['num_train_epochs']):\n","        # Expected completion time for one epoch \n","        epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n","        # Set loss per batch to 0\n","        batch_loss = 0\n","        # Iterate over each batch\n","        for step, batch in enumerate(epoch_iterator):\n","            # Unpack the batch into inputs and attention masks\n","            inputs, attention_masks = batch \n","            # Move features to GPU\n","            inputs = inputs.to(device)\n","            attention_masks = attention_masks.to(device)\n","\n","            # Forward pass: compute model predictions and calculate loss\n","            outputs = model(inputs, attention_mask=attention_masks, labels=inputs)\n","            loss = outputs.loss\n","            # Accumulate the batch loss\n","            batch_loss += loss.item()\n","            # Backward pass: compute gradients\n","            loss.backward()\n","            # Update model parameters and learning rate\n","            optimizer.step()\n","            scheduler.step()\n","            # Reset gradients for the next step\n","            optimizer.zero_grad()\n","        \n","        # Calculate the average loss for the current epoch\n","        avg_train_loss = batch_loss / len(train_dataloader)\n","         # Store the epoch number and average training loss in the training stats list\n","        training_stats.append(\n","            {\n","                'epoch': epoch + 1, # Start at epoch 1 instead of 0\n","                'Training Loss': avg_train_loss,\n","            }\n","        )\n","    # Return the training arguments, statistics, and the fine-tuned model \n","    return args, training_stats, model\n","\n","# Fine-tune gpt2-medium\n","args, training_stats, model = fine_tune_gpt2(train_data=texts)"]},{"cell_type":"markdown","metadata":{},"source":["## Display Model Info"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2024-06-25T18:37:30.523338Z","iopub.status.idle":"2024-06-25T18:37:30.523637Z","shell.execute_reply":"2024-06-25T18:37:30.523499Z","shell.execute_reply.started":"2024-06-25T18:37:30.523487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The GPT-2 model has 292 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","transformer.wte.weight                                  (50259, 1024)\n","transformer.wpe.weight                                  (1024, 1024)\n","\n","==== First Transformer ====\n","\n","transformer.h.0.ln_1.weight                                  (1024,)\n","transformer.h.0.ln_1.bias                                    (1024,)\n","transformer.h.0.attn.c_attn.weight                      (1024, 3072)\n","transformer.h.0.attn.c_attn.bias                             (3072,)\n","transformer.h.0.attn.c_proj.weight                      (1024, 1024)\n","transformer.h.0.attn.c_proj.bias                             (1024,)\n","transformer.h.0.ln_2.weight                                  (1024,)\n","transformer.h.0.ln_2.bias                                    (1024,)\n","transformer.h.0.mlp.c_fc.weight                         (1024, 4096)\n","transformer.h.0.mlp.c_fc.bias                                (4096,)\n","transformer.h.0.mlp.c_proj.weight                       (4096, 1024)\n","transformer.h.0.mlp.c_proj.bias                              (1024,)\n","\n","==== Output Layer ====\n","\n","transformer.ln_f.weight                                      (1024,)\n","transformer.ln_f.bias                                        (1024,)\n"]}],"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:2]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[2:14]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-2:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"markdown","metadata":{},"source":["## Save Fine-Tuned Model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.status.busy":"2024-06-25T18:37:30.524603Z","iopub.status.idle":"2024-06-25T18:37:30.524939Z","shell.execute_reply":"2024-06-25T18:37:30.524772Z","shell.execute_reply.started":"2024-06-25T18:37:30.524759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to ./models/GPT\n"]}],"source":["output_dir = './models/GPT2-medium'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save fine-tuned model and tokenizer\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Save training arguments with fine-tuned model\n","torch.save(args, os.path.join(output_dir, 'training_args.bin'))"]},{"cell_type":"markdown","metadata":{},"source":["## Show training loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.set_option('display.precision', 2)\n","\n","# Print training stats using DataFrame format\n","df = pd.DataFrame(training_stats)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot training loss\n","plt.plot(df['epoch'], df['Training Loss'])\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","plt.savefig(output_dir +'/training_loss.png')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5271245,"sourceId":8771507,"sourceType":"datasetVersion"},{"datasetId":5280684,"sourceId":8784286,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
